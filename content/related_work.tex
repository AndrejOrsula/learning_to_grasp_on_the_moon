\section{Related Work}\label{sec:related-work}

Extraterrestrial environments pose many challenges to both the hardware and software of robotic systems. Even though rovers deployed to Moon and Mars can travel long distances, their actions are controlled remotely by operators from Earth due to their limited autonomous capabilities~\cite{grotzinger_mars_2012}. Current manipulation for planetary exploration relies on using calibrated systems to achieve the required precision. However, their control is based on the manual selection of target waypoints that are acquired from stereo vision~\cite{nickels_vision_2010}. Due to limitations caused by communication delays, an effort has been put into increasing the autonomy of extraterrestrial rovers. One example of an autonomous rover featuring a robotic arm with a mechanical gripper was presented in~\cite{schuster_lru_2016}. With high-level commands, this rover was able to autonomously pick and assemble known objects by planning motions to predefined configurations relative to their pose estimate. More complex mobile manipulation tasks with the same rover were later conducted in a Moon-analogue environment~\cite{lehner_mobile_2018}, albeit the mechanical gripper was replaced with a docking interface to reduce the requirement for high precision positioning by increasing misalignment tolerance. Our work similarly investigates manipulation in lunar environments but focuses on grasping previously unknown objects with a general-purpose mechanical gripper to enable application versatility, despite increasing the task difficulty.

The mobility of rovers with robotic arms extends their reachable workspace, which in turn enhances their capabilities for interacting with the environment. However, the complexity of extraterrestrial environments poses several difficulties due to their unstructured nature and uncertainties caused by limited knowledge of surroundings with imperfect sensory perception. Analytical approaches for robotic grasping lack the required generalization even for terrestrial applications~\cite{sahbani_overview_2012}, despite their effectiveness for task-specific problems~\cite{schuster_lru_2016,lehner_mobile_2018}. Contrary to this, supervised learning provides a way to learn grasp synthesis empirically from labeled datasets. However, this approach requires a large volume of data in order to achieve the desired level of generalization~\cite{mahler_dex-net_2017}. Reinforcement learning enables acquiring policies for sequential decision-making problems by interacting with the environment in a self-supervised manner. Recent research has also applied this method in the space robotics domain for tasks such as planetary soft landing~\cite{xu_deep_2021} and rover path planning~\cite{jin_value_2021}. The application of deep reinforcement learning, i.e. a combination of deep learning and reinforcement learning, has been extensively explored for terrestrial robotic grasping in the last few years~\cite{kroemer_review_2021}. Many of these contributions focus on the final performance using a single object~\cite{popov_data-efficient_2017} or several different objects with simple geometry~\cite{tobin_domain_2017,zeng_learning_2018}. More recent work strives to increase this variety by training on objects with more complex geometry~\cite{gualtieri_pick_2018,kalashnikov_qt-opt_2018}, as it is considered to be one of the most important challenges for learning-based robotic grasping. In~\cite{kalashnikov_qt-opt_2018}, a general policy capable of grasping diverse objects was achieved by training on multiple real robots over the course of several weeks. Our system strives to achieve the same goal while training agents solely inside a simulation. To bridge a possible reality gap per our first contribution, we create a simulation environment with realistic physics and physically-based rendering while relying on domain randomization~\cite{tobin_domain_2017}, which is a popular technique to facilitate the sim-to-real transfer.

Manipulation tasks with high-dimensional continuous action and observation spaces pose many challenges when applying reinforcement learning due to its sample inefficiency, brittleness to hyperparameters, training instability and limited reproducibility~\cite{kroemer_review_2021}. Therefore, several different approaches have been analyzed over the years. Pixel-wise action space has been exploited for selecting action primitives based on traditional motion planning techniques in previous research such as~\cite{zeng_learning_2018}, where a grasp pose synthesis was incorporated with the pushing of objects. Alternative approaches focus on end-to-end learning to directly control the motion of robots either via joint commands~\cite{popov_data-efficient_2017,levine_end--end_2015} or actions that are expressed as Cartesian displacement of the gripper pose~\cite{kalashnikov_qt-opt_2018}. Our proposal similarly employs end-to-end learning of a policy that maps raw observations directly to continuous actions in Cartesian space.

The vast majority of research using deep reinforcement learning for robotic grasping relies on visual image observations coupled with convolutional neural networks. Here, RGB images are commonly used~\cite{tobin_domain_2017,kalashnikov_qt-opt_2018,levine_end--end_2015}, where depth maps or their inclusion in RGB-D data are also common~\cite{zeng_learning_2018}. However, it is argued that 2D convolutional layers do not provide the desired level of generalization over spatial position and orientation for robotic manipulation compared to their well-established generalization over the horizontal and vertical position in the image plane~\cite{gualtieri_pick_2018}. Therefore, our approach employs 3D octree observations due to advancements in their utilization in deep learning~\cite{wang_o-cnn_2017,riegler_octnet_2017}. As opposed to~\cite{wang_o-cnn_2017} that employs octree-based convolutional neural networks to analyze 3D shapes, we utilize octrees for real-time control with deep reinforcement learning. Although~\cite{trasnea_octopath_2021} recently applied octrees to learn planning of trajectories for mobile robots due to their efficiency, there is currently a lack of methods that employ 3D observations for end-to-end learning of manipulation skills. In this way, we study the importance of generalization over the full 6~DOF workspace in which robots operate and compare them to more traditional image-based observations as a part of our second contribution.

The application of this paper is closely related to the work presented in~\cite{grimm_vision-based_2021} and~\cite{wermelinger_grasping_2021}, both of which focus on grasping stones and boulders. Multi-finger humanoid hand is used in~\cite{grimm_vision-based_2021} with computationally-efficient control that enables both grasping and pushing actions. In~\cite{wermelinger_grasping_2021}, an excavator with a two-jaw gripper is used for the autonomous assembly of a large-scale stone wall with the capability to reorient grasped boulders. Whereas these approaches perform 3D object segmentation followed by grasp synthesis and traditional motion planning, our work focuses on end-to-end reinforcement learning. This enables agents to explore the necessary interactions with objects in a self-supervised manner, without the need to manually define complex subroutines such as pushing and reorientation. Furthermore, our focus on lunar environments introduces additional challenges such as uneven terrain and demanding illumination conditions.
